################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  "License"); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################


# This file defines the default environment for Flink's SQL Client.
# Defaults might be overwritten by a session specific environment.


# See the Table API & SQL documentation for details about supported properties.


#==============================================================================
# Tables
#==============================================================================

# Define tables here such as sources, sinks, views, or temporal tables.

tables:
  - name: trnx
    type: source-table
    update-mode: append
    connector:
      property-version: 1
      type: kafka
      version: universal
      topic: transaction.log.1   # kafka topic name
      startup-mode: earliest-offset
      properties:
        - key: bootstrap.servers
          value: <kafka_broker_fqdn>:9092
        - key: group.id
          value: trnx

    format:
      property-version: 1
      type: csv
      schema: "ROW(trnxId LONG, ts LONG, itemId STRING, qty INT)"
      line-delimiter: "\n"
      field-delimiter: "\t"

    schema:
      - name: trnxId
        type: LONG
      - name: ts
        type: LONG
      - name: itemId
        type: STRING
      - name: qty
        type: INT

  - name: query
    type: source-table
    update-mode: append
    connector:
      property-version: 1
      type: kafka
      version: universal
      topic: query.input.log.1   # kafka topic name
      startup-mode: earliest-offset
      properties:
        - key: bootstrap.servers
          value: <kafka_broker_fqdn>:9092
        - key: group.id
          value: query.in

    format:
      property-version: 1
      type: csv
      schema: "ROW(queryId LONG, itemId STRING)"
      line-delimiter: "\n"
      field-delimiter: " "

    schema:
      - name: queryId
        type: LONG
      - name: itemId
        type: STRING

  - name: query_out
    type: source-table
    update-mode: append
    connector:
      property-version: 1
      type: kafka
      version: universal
      topic: query.output.log.1   # kafka topic name
      startup-mode: earliest-offset
      properties:
        - key: bootstrap.servers
          value: <kafka_broker_fqdn>:9092
        - key: group.id
          value: query.out

    format:
      property-version: 1
      type: csv
      schema: "ROW(queryId LONG, itemId STRING, qty INT)"
      line-delimiter: "\n"
      field-delimiter: " "

    schema:
      - name: queryId
        type: LONG
      - name: itemId
        type: STRING
      - name: qty
        type: INT


#==============================================================================
# User-defined functions
#==============================================================================

# Define scalar, aggregate, or table functions here.

functions: [] # empty list

#==============================================================================
# Catalogs
#==============================================================================

# Define catalogs here.

catalogs: [] # empty list

#==============================================================================
# Execution properties
#==============================================================================

# Properties that change the fundamental execution behavior of a table program.

execution:
  # select the implementation responsible for planning table programs
  # possible values are 'old' (used by default) or 'blink'
  planner: blink
  # 'batch' or 'streaming' execution
  type: streaming
  # allow 'event-time' or only 'processing-time' in sources
  time-characteristic: event-time
  # interval in ms for emitting periodic watermarks
  periodic-watermarks-interval: 200
  # 'changelog' or 'table' presentation of results
  result-mode: table
  # maximum number of maintained rows in 'table' presentation of results
  max-table-result-rows: 1000000
  # parallelism of the program
  parallelism: 1
  # maximum parallelism
  max-parallelism: 128
  # minimum idle state retention in ms
  min-idle-state-retention: 0
  # maximum idle state retention in ms
  max-idle-state-retention: 0
  # current catalog ('default_catalog' by default)
  current-catalog: default_catalog
  # current database of the current catalog (default database of the catalog by default)
  current-database: default_database
  # controls how table programs are restarted in case of a failures
  restart-strategy:
    # strategy type
    # possible values are "fixed-delay", "failure-rate", "none", or "fallback" (default)
    type: fallback

#==============================================================================
# Deployment properties
#==============================================================================

# Properties that describe the cluster to which table programs are submitted to.

deployment:
  # general cluster communication timeout in ms
  response-timeout: 5000
  # (optional) address from cluster to gateway
  gateway-address: ""
  # (optional) port from cluster to gateway
  gateway-port: 0
